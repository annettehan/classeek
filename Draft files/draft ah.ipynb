{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: joblib in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: funcy in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: setuptools in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (50.3.2.post20201201)\n",
      "Requirement already satisfied: numexpr in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: sklearn in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: future in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: gensim in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.23.2)\n",
      "Requirement already satisfied: scipy in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: scipy in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.5.2)\n",
      "Requirement already satisfied: joblib in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.23.2)\n",
      "Requirement already satisfied: requests in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (2.25.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/annetteh/opt/anaconda3/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annetteh/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, RE_TAGS, RE_PUNCT, RE_WHITESPACE\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv('assets/original/2021-10-19-MichiganOnline-courses.csv')\n",
    "f_21 = pd.read_csv('assets/f_21_merge.csv')\n",
    "w_22 = pd.read_csv('assets/w_22_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Through this course, you will start by address...\n",
       "1      The third and the last course of the Addressin...\n",
       "2      Are you concerned about climate change? Would ...\n",
       "3      This course, Additive Manufacturing, is the th...\n",
       "4      This course builds upon the fundamental concep...\n",
       "                             ...                        \n",
       "516    This module examines the impacts of incarcerat...\n",
       "517    This third course in the “Good with Words: Wri...\n",
       "518    This fourth and final course in the “Good with...\n",
       "519    This second course in the Good with Words: Wri...\n",
       "520    This course will teach you how to use your wri...\n",
       "Name: description, Length: 521, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        This course seeks to introduce students to eve...\n",
       "1        This seminar introduces first-year students to...\n",
       "2        This class explores the fascinating shift in B...\n",
       "3        This seminar introduces first-year students to...\n",
       "4        This seminar introduces first-year students to...\n",
       "                               ...                        \n",
       "74540                                                  NaN\n",
       "74541                                                  NaN\n",
       "74542                                                  NaN\n",
       "74543                                                  NaN\n",
       "74544                                                  NaN\n",
       "Name: description, Length: 74545, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_22['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tokenize_text(tweet_df):\n",
    "\n",
    "#     tweet_df['description'] = tweet_df['description'].astype(str)\n",
    "    \n",
    "#     tweet_df['tokens'] = [word for word in tweet_df['description'].str.split()]\n",
    "\n",
    "\n",
    "\n",
    "#     STOPWORDS_list = [x for x in STOPWORDS]\n",
    "\n",
    "# #     def remove_stopwords(tokens, stopwords=custom_stop_words + STOPWORDS_list):\n",
    "# #         return [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "#     def remove_stopwords(tokens, stopwords=STOPWORDS_list):\n",
    "#         return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "#     tweet_df['tokens'] = tweet_df['tokens'].apply(lambda x: remove_stopwords(x))\n",
    "    \n",
    "#     tweet_df['tokens'] = tweet_df['tokens'].apply(lambda x: preprocess_string(x))\n",
    "    \n",
    "#     return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean_tokenize_text(w_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model_for_each_df(df, num_topics):\n",
    "    \n",
    "    data = df['description'].dropna()\n",
    "    \n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "            \n",
    "        return texts_out\n",
    "    \n",
    "    lemmatized_texts = lemmatization(data)\n",
    "    \n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return final\n",
    "    \n",
    "    \n",
    "\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "    \n",
    "    common_words = Counter([item for sublist in data_words for item in sublist]).most_common(200)\n",
    "    \n",
    "    custom_stop_words = [word_count[0] for word_count in common_words]\n",
    "    \n",
    "    return custom_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = lda_model_for_each_df(w_22, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "# TF-IDF removal\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sublist in data_words:\n",
    "    sublist = [item for item in sublist if item in custom_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for sublist in data_words for item in sublist if item not in custom_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def lda_model_for_each_df(df, num_topics):\n",
    "    \n",
    "    data = df['description'].dropna()\n",
    "    \n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        return (texts_out)\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "\n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return (final)\n",
    "\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "\n",
    "    # Create bigrams and trigrams\n",
    "    \n",
    "    bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return([bigram[doc] for doc in texts])\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "    data_bigrams = make_bigrams(data_words)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "    # TF-IDF removal\n",
    "    \n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.05\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words + words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus[:-1],\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=\"auto\")\n",
    "\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100 = Counter([item for sublist in data_words for item in sublist]).most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [i[0] for i in top_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model_for_each_df(df, num_topics):\n",
    "    \n",
    "    data = df['description'].dropna()\n",
    "    \n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        return (texts_out)\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "\n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return (final)\n",
    "\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "\n",
    "    # Create bigrams and trigrams\n",
    "    \n",
    "    bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=50)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return([bigram[doc] for doc in texts])\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "    data_bigrams = make_bigrams(data_words)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "    # TF-IDF removal\n",
    "    \n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.05\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words + words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus[:-1],\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=num_topics,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=\"auto\")\n",
    "\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_for_each_df(f_21, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
