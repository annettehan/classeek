{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/rwalk/gsdmm.git ## install gsdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv('assets/original/2021-10-19-MichiganOnline-courses.csv')\n",
    "f_21 = pd.read_csv('assets/f_21_merge.csv')\n",
    "w_22 = pd.read_csv('assets/w_22_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsdmm_model_for_each_df(df, num_topics=10):\n",
    "    \n",
    "    data = df['description'].dropna()\n",
    "    \n",
    "\n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        return (texts_out)\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "\n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return (final)\n",
    "\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "\n",
    "    # Create bigrams and trigrams\n",
    "    bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return([bigram[doc] for doc in texts])\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "    data_bigrams = make_bigrams(data_words)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "    # TF-IDF removal\n",
    "\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.03\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "\n",
    "\n",
    "    mgp = MovieGroupProcess(K=10, alpha=0.01, beta=0.01, n_iters=30)\n",
    "\n",
    "    vocab = set(x for t in texts for x in t)\n",
    "    n_terms = len(vocab)\n",
    "    model = mgp.fit(texts, n_terms)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def top_words(cluster_word_distribution, top_cluster, values):\n",
    "        for cluster in top_cluster:\n",
    "            sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "            print(\"\\nCluster %s : %s\"%(cluster,sort_dicts))\n",
    "\n",
    "\n",
    "    doc_count = np.array(mgp.cluster_doc_count)\n",
    "    print('Number of documents per topic :', doc_count)\n",
    "    print('*'*20)\n",
    "    # Topics sorted by the number of document they are allocated to\n",
    "    top_index = doc_count.argsort()[-10:][::-1]\n",
    "    print('Most important clusters (by number of docs inside):', top_index)\n",
    "    print('*'*20)\n",
    "    # Show the top 5 words in term frequency for each cluster \n",
    "    top_words(mgp.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 15255 clusters with 10 clusters populated\n",
      "In stage 1: transferred 1525 clusters with 10 clusters populated\n",
      "In stage 2: transferred 389 clusters with 10 clusters populated\n",
      "In stage 3: transferred 292 clusters with 10 clusters populated\n",
      "In stage 4: transferred 201 clusters with 10 clusters populated\n",
      "In stage 5: transferred 178 clusters with 10 clusters populated\n",
      "In stage 6: transferred 159 clusters with 10 clusters populated\n",
      "In stage 7: transferred 137 clusters with 10 clusters populated\n",
      "In stage 8: transferred 148 clusters with 10 clusters populated\n",
      "In stage 9: transferred 145 clusters with 10 clusters populated\n",
      "In stage 10: transferred 139 clusters with 10 clusters populated\n",
      "In stage 11: transferred 131 clusters with 10 clusters populated\n",
      "In stage 12: transferred 144 clusters with 10 clusters populated\n",
      "In stage 13: transferred 152 clusters with 10 clusters populated\n",
      "In stage 14: transferred 139 clusters with 10 clusters populated\n",
      "In stage 15: transferred 153 clusters with 10 clusters populated\n",
      "In stage 16: transferred 134 clusters with 10 clusters populated\n",
      "In stage 17: transferred 145 clusters with 10 clusters populated\n",
      "In stage 18: transferred 144 clusters with 10 clusters populated\n",
      "In stage 19: transferred 149 clusters with 10 clusters populated\n",
      "In stage 20: transferred 144 clusters with 10 clusters populated\n",
      "In stage 21: transferred 134 clusters with 10 clusters populated\n",
      "In stage 22: transferred 137 clusters with 10 clusters populated\n",
      "In stage 23: transferred 148 clusters with 10 clusters populated\n",
      "In stage 24: transferred 145 clusters with 10 clusters populated\n",
      "In stage 25: transferred 131 clusters with 10 clusters populated\n",
      "In stage 26: transferred 135 clusters with 10 clusters populated\n",
      "In stage 27: transferred 125 clusters with 10 clusters populated\n",
      "In stage 28: transferred 133 clusters with 10 clusters populated\n",
      "In stage 29: transferred 134 clusters with 10 clusters populated\n",
      "Number of documents per topic : [1237 1256 1872 1748 2158 2199 1226 1499 1959 2861]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [9 5 4 8 2 3 7 1 0 6]\n",
      "********************\n",
      "\n",
      "Cluster 9 : [('course', 8201), ('student', 7272), ('lab', 3887), ('class', 3348), ('include', 1857), ('language', 1777), ('work', 1706), ('use', 1690), ('exam', 1656), ('make', 1638), ('laboratory', 1504), ('first', 1370), ('online', 1351), ('take', 1279), ('final', 1259), ('other', 1214), ('history', 1213), ('computer', 1158), ('time', 1138), ('reading', 1119)]\n",
      "\n",
      "Cluster 5 : [('student', 3818), ('course', 2510), ('science', 1741), ('term', 1680), ('topic', 1291), ('chemistry', 1221), ('concept', 1181), ('engineering', 1035), ('credit', 1020), ('more', 985), ('seat', 972), ('periodically', 972), ('interest', 970), ('section', 944), ('lecture', 930), ('major', 924), ('complete', 877), ('take', 870), ('once', 846), ('first', 828)]\n",
      "\n",
      "Cluster 4 : [('student', 2236), ('spanish', 1320), ('course', 1040), ('report', 1000), ('work', 984), ('problem', 923), ('design', 891), ('such', 889), ('write', 874), ('research', 763), ('presentation', 702), ('present', 696), ('oral', 694), ('level', 656), ('hour', 625), ('include', 614), ('instructor', 597), ('eec', 580), ('engineering', 561), ('review', 541)]\n",
      "\n",
      "Cluster 8 : [('student', 1116), ('course', 915), ('theatre', 915), ('work', 835), ('project', 766), ('faculty', 745), ('production', 721), ('supervision', 715), ('craft', 552), ('principle', 542), ('various', 484), ('research', 475), ('use', 396), ('learn', 376), ('cover', 373), ('individual', 371), ('film', 367), ('practice', 322), ('explore', 317), ('exploration', 317)]\n",
      "\n",
      "Cluster 2 : [('student', 2870), ('provide', 1998), ('listening', 1786), ('skill', 1707), ('reading', 1586), ('culture', 1557), ('course', 1482), ('language', 1433), ('speak', 1224), ('speaking', 1190), ('write', 1085), ('design', 1059), ('spanish', 1026), ('integrate', 1026), ('people', 1006), ('active', 988), ('early', 985), ('opportunity', 974), ('intend', 972), ('challenge', 972)]\n",
      "\n",
      "Cluster 3 : [('practice', 3885), ('activity', 3732), ('course', 3129), ('topic', 2790), ('hispanic', 2760), ('student', 2643), ('reading', 2622), ('culture', 2508), ('material', 2484), ('vocabulary', 2484), ('situation', 2484), ('grammatical', 2484), ('world', 1728), ('language', 1518), ('politic', 1476), ('function', 1472), ('speak', 1472), ('history', 1457), ('include', 1419), ('society', 1404)]\n",
      "\n",
      "Cluster 7 : [('course', 1804), ('student', 962), ('use', 660), ('study', 573), ('other', 566), ('class', 538), ('include', 501), ('biology', 456), ('provide', 455), ('topic', 441), ('cultural', 440), ('analysis', 380), ('human', 380), ('introduction', 362), ('work', 343), ('process', 338), ('history', 326), ('knowledge', 305), ('term', 304), ('world', 301)]\n",
      "\n",
      "Cluster 1 : [('student', 1957), ('course', 1863), ('first', 622), ('language', 595), ('year', 574), ('skill', 554), ('class', 504), ('design', 480), ('writing', 477), ('learn', 406), ('make', 394), ('include', 386), ('reading', 381), ('build', 366), ('social', 330), ('basic', 325), ('speak', 322), ('community', 306), ('experience', 302), ('japanese', 300)]\n",
      "\n",
      "Cluster 0 : [('student', 1629), ('design', 1060), ('datum', 1043), ('course', 1031), ('material', 937), ('focus', 923), ('make', 923), ('laboratory', 921), ('hour', 919), ('understand', 915), ('base', 915), ('weekly', 912), ('result', 910), ('analyze', 905), ('observation', 905), ('accompany', 903), ('compare', 903), ('prediction', 903), ('theory', 758), ('introduce', 746)]\n",
      "\n",
      "Cluster 6 : [('course', 2708), ('student', 2659), ('academic', 2095), ('essay', 2017), ('reading', 1650), ('well', 1639), ('writing', 1491), ('genre', 1404), ('argument', 1356), ('question', 1344), ('work', 1315), ('variety', 928), ('creative', 921), ('dance', 840), ('skill', 825), ('develop', 822), ('also', 812), ('explore', 723), ('learn', 711), ('context', 709)]\n"
     ]
    }
   ],
   "source": [
    "gsdmm_model_for_each_df(f_21, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
