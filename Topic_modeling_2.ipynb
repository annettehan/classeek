{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/rwalk/gsdmm.git ## install gsdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv('assets/original/2021-10-19-MichiganOnline-courses.csv')\n",
    "f_21 = pd.read_csv('assets/f_21_merge.csv')\n",
    "w_22 = pd.read_csv('assets/w_22_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsdmm_model_for_each_df(df, num_topics):\n",
    "\n",
    "    data = df['description'].dropna()\n",
    "\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\"]): #trying only noun instead [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            #final = \" \".join(new_text)\n",
    "            texts_out.append(new_text)\n",
    "        return (texts_out)\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "\n",
    "    def stop_word_removal(texts):\n",
    "\n",
    "        flat_texts = [t for text in texts for t in text]\n",
    "        common_words = [k for k,v in Counter(flat_texts).most_common(10)]\n",
    "        texts_out = []\n",
    "        stop = set(stopwords.words('english')+common_words)\n",
    "        for text in texts:\n",
    "            new_text = [t for t in text if t.lower() not in stop]\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        return (texts_out)\n",
    "\n",
    "    stop_word_removed_texts = stop_word_removal(lemmatized_texts)\n",
    "\n",
    "\n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return (final)\n",
    "\n",
    "    data_words = gen_words(stop_word_removed_texts)\n",
    "\n",
    "\n",
    "    # Create bigrams and trigrams\n",
    "    bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return([bigram[doc] for doc in texts])\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "    data_bigrams = make_bigrams(data_words)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "    \n",
    "    \n",
    "    # TF-IDF removal - not used here\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.03\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "    #\n",
    "\n",
    "\n",
    "    #mgp topic modeling\n",
    "    mgp = MovieGroupProcess(K=num_topics, alpha=0.01, beta=0.01, n_iters=30)\n",
    "\n",
    "    vocab = set(x for t in texts for x in t)\n",
    "    n_terms = len(vocab)\n",
    "    model = mgp.fit(texts, n_terms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def top_words(cluster_word_distribution, top_cluster, values):\n",
    "        for cluster in top_cluster:\n",
    "            sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "            print(\"\\nCluster %s : %s\"%(cluster,sort_dicts))\n",
    "\n",
    "\n",
    "    doc_count = np.array(mgp.cluster_doc_count)\n",
    "    print('Number of documents per topic :', doc_count)\n",
    "    print('*'*20)\n",
    "    # Topics sorted by the number of document they are allocated to\n",
    "    top_index = doc_count.argsort()[-10:][::-1]\n",
    "    print('Most important clusters (by number of docs inside):', top_index)\n",
    "    print('*'*20)\n",
    "    # Show the top 5 words in term frequency for each cluster \n",
    "        \n",
    "    top_words(mgp.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 16164 clusters with 20 clusters populated\n",
      "In stage 1: transferred 1904 clusters with 20 clusters populated\n",
      "In stage 2: transferred 710 clusters with 20 clusters populated\n",
      "In stage 3: transferred 493 clusters with 20 clusters populated\n",
      "In stage 4: transferred 374 clusters with 20 clusters populated\n",
      "In stage 5: transferred 357 clusters with 20 clusters populated\n",
      "In stage 6: transferred 381 clusters with 20 clusters populated\n",
      "In stage 7: transferred 334 clusters with 20 clusters populated\n",
      "In stage 8: transferred 330 clusters with 20 clusters populated\n",
      "In stage 9: transferred 335 clusters with 20 clusters populated\n",
      "In stage 10: transferred 290 clusters with 20 clusters populated\n",
      "In stage 11: transferred 324 clusters with 20 clusters populated\n",
      "In stage 12: transferred 327 clusters with 20 clusters populated\n",
      "In stage 13: transferred 308 clusters with 20 clusters populated\n",
      "In stage 14: transferred 314 clusters with 20 clusters populated\n",
      "In stage 15: transferred 308 clusters with 20 clusters populated\n",
      "In stage 16: transferred 297 clusters with 20 clusters populated\n",
      "In stage 17: transferred 309 clusters with 20 clusters populated\n",
      "In stage 18: transferred 334 clusters with 20 clusters populated\n",
      "In stage 19: transferred 335 clusters with 20 clusters populated\n",
      "In stage 20: transferred 294 clusters with 20 clusters populated\n",
      "In stage 21: transferred 307 clusters with 20 clusters populated\n",
      "In stage 22: transferred 304 clusters with 20 clusters populated\n",
      "In stage 23: transferred 299 clusters with 20 clusters populated\n",
      "In stage 24: transferred 282 clusters with 20 clusters populated\n",
      "In stage 25: transferred 279 clusters with 20 clusters populated\n",
      "In stage 26: transferred 298 clusters with 20 clusters populated\n",
      "In stage 27: transferred 294 clusters with 20 clusters populated\n",
      "In stage 28: transferred 297 clusters with 20 clusters populated\n",
      "In stage 29: transferred 300 clusters with 20 clusters populated\n",
      "Number of documents per topic : [1188 1307  449  958  308 1258 1253 1106  727 1898  447  819  621  416\n",
      " 1082  531  755  923  551 1418]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [ 9 19  1  5  6  0  7 14  3 17]\n",
      "********************\n",
      "\n",
      "Cluster 9 : [('engineering', 1267), ('laboratory', 984), ('lecture', 852), ('interest', 808), ('computer', 806), ('science', 751), ('seminar', 687), ('faculty', 569), ('credit', 529), ('report', 497), ('work', 491), ('research', 396), ('group', 380), ('physics', 360), ('field', 335), ('study', 331), ('description', 261), ('idea', 255), ('project', 254), ('presentation', 252)]\n",
      "\n",
      "Cluster 19 : [('listening', 1434), ('skill', 1212), ('people', 1076), ('user', 1018), ('challenge', 998), ('learning', 981), ('literature', 980), ('speaking', 978), ('opportunity', 972), ('insight', 962), ('structure', 470), ('vocabulary', 456), ('assignment', 266), ('history', 250), ('film', 249), ('detail', 240), ('future', 230), ('sentence', 224), ('repetition', 224), ('comparison', 224)]\n",
      "\n",
      "Cluster 1 : [('practice', 2484), ('situation', 2484), ('lecture', 1276), ('society', 1253), ('history', 1248), ('listening', 1246), ('world', 1244), ('video', 1244), ('issue', 1244), ('understanding', 1244), ('politic', 1244), ('aim', 1244), ('computer', 1244), ('article', 1244), ('newspaper', 1244), ('relevance', 1244), ('time', 1242), ('function', 1242), ('tool', 1242), ('content', 1242)]\n",
      "\n",
      "Cluster 5 : [('theatre', 936), ('faculty', 803), ('production', 578), ('supervision', 541), ('craft', 538), ('practice', 466), ('principle', 323), ('exploration', 317), ('university', 273), ('level', 263), ('staff', 249), ('skill', 236), ('project', 227), ('association', 227), ('continuation', 223), ('practicum', 221), ('history', 181), ('design', 152), ('world', 133), ('theory', 129)]\n",
      "\n",
      "Cluster 6 : [('lab', 4416), ('laboratory', 2289), ('datum', 1473), ('hour', 1441), ('science', 1201), ('principle', 1180), ('observation', 1172), ('result', 1163), ('prediction', 1163), ('physics', 1137), ('method', 932), ('information', 780), ('instructor', 780), ('mechanic', 644), ('theory', 628), ('law', 592), ('experiment', 574), ('electricity', 554), ('magnetism', 554), ('classroom', 529)]\n",
      "\n",
      "Cluster 0 : [('history', 586), ('research', 401), ('world', 396), ('century', 343), ('life', 333), ('project', 329), ('text', 327), ('sport', 325), ('skill', 321), ('question', 319), ('story', 310), ('film', 277), ('way', 275), ('time', 266), ('experience', 252), ('group', 243), ('literature', 235), ('discussion', 231), ('past', 218), ('year', 197)]\n",
      "\n",
      "Cluster 7 : [('term', 1270), ('chemistry', 1030), ('problem', 983), ('biology', 719), ('hour', 590), ('interest', 583), ('eec', 577), ('credit', 553), ('science', 531), ('area', 495), ('research', 479), ('design', 429), ('opportunity', 425), ('sequence', 383), ('major', 352), ('requirement', 328), ('week', 317), ('graduate', 243), ('report', 240), ('presentation', 240)]\n",
      "\n",
      "Cluster 14 : [('introduction', 649), ('analysis', 593), ('theory', 541), ('algorithm', 463), ('datum', 417), ('system', 266), ('science', 256), ('function', 248), ('computer', 242), ('problem', 224), ('probability', 222), ('method', 218), ('structure', 199), ('tree', 199), ('design', 198), ('data', 181), ('notation', 179), ('technique', 171), ('programming', 168), ('application', 161)]\n",
      "\n",
      "Cluster 3 : [('skill', 805), ('listening', 703), ('video', 478), ('world', 414), ('film', 383), ('aspect', 345), ('year', 325), ('sequence', 294), ('food', 277), ('grammar', 248), ('structure', 215), ('century', 202), ('speaking', 194), ('half', 192), ('technique', 191), ('credit', 188), ('major', 188), ('part', 185), ('supervision', 179), ('area', 172)]\n",
      "\n",
      "Cluster 17 : [('history', 483), ('work', 393), ('gender', 334), ('study', 327), ('community', 306), ('practice', 287), ('research', 280), ('race', 239), ('skill', 230), ('process', 222), ('medium', 199), ('art', 183), ('level', 180), ('term', 178), ('addition', 156), ('issue', 150), ('identity', 149), ('change', 147), ('discipline', 146), ('attention', 146)]\n"
     ]
    }
   ],
   "source": [
    "gsdmm_model_for_each_df(f_21, num_topics=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
