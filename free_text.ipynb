{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13c99c3",
   "metadata": {},
   "source": [
    "## This notebook:\n",
    "- Recommend a course based on free text the user inputs (this is more like an Information Retrieval system, research engine, because the user input is free text, like Google search)\n",
    "1.    \n",
    "    - User enters a block of text (free text, no preset options)\n",
    "    - This block of text would describe their interest\n",
    "    - Process this block of text\n",
    "    - Find the topics of this block of text\n",
    "    \n",
    "2.    \n",
    "    - Process text of the descriptions of courses\n",
    "    - Find the topics of those blocks of text\n",
    "    \n",
    "3.    \n",
    "    - Match the topic of user's input text and all the courses\n",
    "    - Compute similarity scores\n",
    "    - Rank these scores from high to low\n",
    "    - Return the n number of recommendations needed by order of similarity\n",
    "    \n",
    "- Need to improve\n",
    "\n",
    "    - Text processing\n",
    "    - Topic modeling (the recommendations are not quite logical yet since the text processing and topic modeling are not quite well-done yet)\n",
    "    - This is my test using fuzzywuzzy, a very simple text matching library. It doesn't work since it only based on the text itself and not the semantic of the text.\n",
    "    - We need to look at similary of documents based on their semantic, word embedding\n",
    "    - Look at Hellinger distance https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_distance_metrics.html\n",
    "    - Look at NLP using Deep Learning in Python https://towardsdatascience.com/deep-learning-for-semantic-text-matching-d4df6c2cf4c5 (seems cool, detailed notebook and youtube tutorial)\n",
    "    - Look at NLP using RNN (word2vec) https://towardsdatascience.com/text-matching-with-deep-learning-e6aa05333399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521d351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bf5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv('assets/original/2021-10-19-MichiganOnline-courses.csv')\n",
    "f_21 = pd.read_csv('assets/f_21_merge.csv')\n",
    "w_22 = pd.read_csv('assets/w_22_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "144c0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = 'korean history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecd94990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(user_text, df):\n",
    "    \n",
    "    df.drop_duplicates(subset=['course'], inplace=True)\n",
    "    df.dropna(subset=['description'], inplace=True)\n",
    "    df.fillna('', inplace=True)\n",
    "    \n",
    "    des_list = []\n",
    "    course_title_list = []\n",
    "    score_list = []\n",
    "    course_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        des = row['description']\n",
    "        course_title = row['Course Title']\n",
    "        course = row['course']\n",
    "        score = fuzz.ratio(user_text, des)\n",
    "\n",
    "        des_list.append(des)\n",
    "        course_title_list.append(course_title)\n",
    "        score_list.append(score)\n",
    "        course_list.append(course)\n",
    "\n",
    "    score_df = pd.DataFrame({'user_text': user_text, 'course_des': des_list, 'course': course_list,\n",
    "                             'course_title': course_title_list, 'score': score_list})\n",
    "    \n",
    "    score_df = score_df[score_df['score'] >= 20]\n",
    "\n",
    "    return score_df.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc71eca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_text</th>\n",
       "      <th>course_des</th>\n",
       "      <th>course</th>\n",
       "      <th>course_title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Independent study.</td>\n",
       "      <td>MUSICOL 481</td>\n",
       "      <td>Special Projects</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Course leads to THEORY 236.</td>\n",
       "      <td>THEORY 135</td>\n",
       "      <td>Intr Mus Thry</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Applied Statistics II</td>\n",
       "      <td>DATASCI 501</td>\n",
       "      <td>Applied Stat II</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Topics of current interest selected by the fac...</td>\n",
       "      <td>EECS 198</td>\n",
       "      <td>Special Topics</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Individual work and reading for graduate stude...</td>\n",
       "      <td>THEORY 570</td>\n",
       "      <td>Directed Indiv Study</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Regular reports and conferences required.</td>\n",
       "      <td>CLARCH 499</td>\n",
       "      <td>Supervised Reading</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Special topics that vary from term to term.</td>\n",
       "      <td>THEORY 407</td>\n",
       "      <td>Directed Indiv Stdy</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Theories of Pictorial Autonomy: Writing About ...</td>\n",
       "      <td>HISTART 402</td>\n",
       "      <td>Cont Interp in A H</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Selected topics pertinent to mechanical engine...</td>\n",
       "      <td>MECHENG 499</td>\n",
       "      <td>Spec Topics in M E</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Individual work and reading for undergraduate ...</td>\n",
       "      <td>THTREMUS 400</td>\n",
       "      <td>Directed Reading</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Reading original works in selected areas of Cz...</td>\n",
       "      <td>CZECH 480</td>\n",
       "      <td>Superv Czech Read</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Introduction to Urban and Environmental Planning</td>\n",
       "      <td>URP 423</td>\n",
       "      <td>Int U P&amp;Env</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>korean history</td>\n",
       "      <td>The Art of Podcasting</td>\n",
       "      <td>DIGITAL 200</td>\n",
       "      <td>Dig Media Writing</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>korean history</td>\n",
       "      <td>The Art of Podcasting</td>\n",
       "      <td>WRITING 200</td>\n",
       "      <td>Dig Media Writing</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Administered by ICCS, students take courses in...</td>\n",
       "      <td>STDABRD 353</td>\n",
       "      <td>Classical Studies IT</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Selected topics pertinent to Civil and Environ...</td>\n",
       "      <td>CEE 501</td>\n",
       "      <td>Special Topics CEE</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>korean history</td>\n",
       "      <td>An independent senior Honors reading course fo...</td>\n",
       "      <td>LING 495</td>\n",
       "      <td>Senior Honors Read</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Continuation of THTREMUS 274, leading to SAFD ...</td>\n",
       "      <td>THTREMUS 374</td>\n",
       "      <td>Stage Combat II</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>korean history</td>\n",
       "      <td>(Remote, Synchronous)</td>\n",
       "      <td>SOC 320</td>\n",
       "      <td>IGR Facilitation</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>korean history</td>\n",
       "      <td>(Remote, Synchronous)</td>\n",
       "      <td>PSYCH 310</td>\n",
       "      <td>IGR Facilitation</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Special Topics in Naval Architecture and Marin...</td>\n",
       "      <td>NAVARCH 599</td>\n",
       "      <td>Special Topics NAME</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>korean history</td>\n",
       "      <td>(Remote, Synchronous)</td>\n",
       "      <td>ALA 320</td>\n",
       "      <td>IGR Facilitation</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>korean history</td>\n",
       "      <td>What does an American Jewish novelist write ab...</td>\n",
       "      <td>JUDAIC 389</td>\n",
       "      <td>Jewish Literature</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Independent research under supervision of facu...</td>\n",
       "      <td>HONORS 390</td>\n",
       "      <td>Jr Honors Research</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>korean history</td>\n",
       "      <td>What does an American Jewish novelist write ab...</td>\n",
       "      <td>ENGLISH 383</td>\n",
       "      <td>Jewish Literature</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Study of traditional dances of the African Congo.</td>\n",
       "      <td>DANCE 262</td>\n",
       "      <td>Congolese Dance I</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Senior thesis research. Under the supervision ...</td>\n",
       "      <td>JUDAIC 497</td>\n",
       "      <td>Senior Thesis</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Undergraduate students may work independently ...</td>\n",
       "      <td>HISTART 399</td>\n",
       "      <td>Independent Study</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>korean history</td>\n",
       "      <td>From Wikipedia:</td>\n",
       "      <td>ENGLISH 364</td>\n",
       "      <td>Contemporary Novel</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>korean history</td>\n",
       "      <td>“Don’t read the comments.” We’re going to igno...</td>\n",
       "      <td>DIGITAL 201</td>\n",
       "      <td>Dig Media Wrtg Mini</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>korean history</td>\n",
       "      <td>“Don’t read the comments.” We’re going to igno...</td>\n",
       "      <td>WRITING 201</td>\n",
       "      <td>Dig Media Wrtg Mini</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Selected topics of current interest in industr...</td>\n",
       "      <td>IOE 591</td>\n",
       "      <td>Special Topics</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Selected topics of current interest in industr...</td>\n",
       "      <td>IOE 491</td>\n",
       "      <td>Spec Top Ind Engr</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>korean history</td>\n",
       "      <td>A seminar on topics in quaternary geology or g...</td>\n",
       "      <td>EARTH 532</td>\n",
       "      <td>Sem Climate-Tect-Top</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>korean history</td>\n",
       "      <td>Undergraduate research in geology for students...</td>\n",
       "      <td>EARTH 299</td>\n",
       "      <td>Ind Study EARTH</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>korean history</td>\n",
       "      <td>A study of the principles of scenery drafting.</td>\n",
       "      <td>THTREMUS 462</td>\n",
       "      <td>Drafting</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_text                                         course_des  \\\n",
       "1298  korean history                                 Independent study.   \n",
       "1797  korean history                        Course leads to THEORY 236.   \n",
       "503   korean history                              Applied Statistics II   \n",
       "627   korean history  Topics of current interest selected by the fac...   \n",
       "1807  korean history  Individual work and reading for graduate stude...   \n",
       "439   korean history          Regular reports and conferences required.   \n",
       "1803  korean history        Special topics that vary from term to term.   \n",
       "945   korean history  Theories of Pictorial Autonomy: Writing About ...   \n",
       "1202  korean history  Selected topics pertinent to mechanical engine...   \n",
       "1830  korean history  Individual work and reading for undergraduate ...   \n",
       "493   korean history  Reading original works in selected areas of Cz...   \n",
       "1867  korean history   Introduction to Urban and Environmental Planning   \n",
       "504   korean history                              The Art of Podcasting   \n",
       "1902  korean history                              The Art of Podcasting   \n",
       "1742  korean history  Administered by ICCS, students take courses in...   \n",
       "355   korean history  Selected topics pertinent to Civil and Environ...   \n",
       "1133  korean history  An independent senior Honors reading course fo...   \n",
       "1826  korean history  Continuation of THTREMUS 274, leading to SAFD ...   \n",
       "1645  korean history                             (Remote, Synchronous)    \n",
       "1498  korean history                             (Remote, Synchronous)    \n",
       "1321  korean history  Special Topics in Naval Architecture and Marin...   \n",
       "86    korean history                             (Remote, Synchronous)    \n",
       "1084  korean history  What does an American Jewish novelist write ab...   \n",
       "1010  korean history  Independent research under supervision of facu...   \n",
       "753   korean history  What does an American Jewish novelist write ab...   \n",
       "495   korean history  Study of traditional dances of the African Congo.   \n",
       "1088  korean history  Senior thesis research. Under the supervision ...   \n",
       "944   korean history  Undergraduate students may work independently ...   \n",
       "750   korean history                                    From Wikipedia:   \n",
       "505   korean history  “Don’t read the comments.” We’re going to igno...   \n",
       "1903  korean history  “Don’t read the comments.” We’re going to igno...   \n",
       "1056  korean history  Selected topics of current interest in industr...   \n",
       "1052  korean history  Selected topics of current interest in industr...   \n",
       "565   korean history  A seminar on topics in quaternary geology or g...   \n",
       "542   korean history  Undergraduate research in geology for students...   \n",
       "1841  korean history     A study of the principles of scenery drafting.   \n",
       "\n",
       "            course          course_title  score  \n",
       "1298   MUSICOL 481      Special Projects     38  \n",
       "1797    THEORY 135         Intr Mus Thry     34  \n",
       "503    DATASCI 501       Applied Stat II     29  \n",
       "627       EECS 198        Special Topics     28  \n",
       "1807    THEORY 570  Directed Indiv Study     25  \n",
       "439     CLARCH 499    Supervised Reading     25  \n",
       "1803    THEORY 407   Directed Indiv Stdy     25  \n",
       "945    HISTART 402    Cont Interp in A H     24  \n",
       "1202   MECHENG 499    Spec Topics in M E     24  \n",
       "1830  THTREMUS 400      Directed Reading     24  \n",
       "493      CZECH 480     Superv Czech Read     24  \n",
       "1867       URP 423           Int U P&Env     23  \n",
       "504    DIGITAL 200     Dig Media Writing     23  \n",
       "1902   WRITING 200     Dig Media Writing     23  \n",
       "1742   STDABRD 353  Classical Studies IT     23  \n",
       "355        CEE 501    Special Topics CEE     23  \n",
       "1133      LING 495    Senior Honors Read     23  \n",
       "1826  THTREMUS 374       Stage Combat II     22  \n",
       "1645       SOC 320      IGR Facilitation     22  \n",
       "1498     PSYCH 310      IGR Facilitation     22  \n",
       "1321   NAVARCH 599   Special Topics NAME     22  \n",
       "86         ALA 320      IGR Facilitation     22  \n",
       "1084    JUDAIC 389     Jewish Literature     22  \n",
       "1010    HONORS 390    Jr Honors Research     22  \n",
       "753    ENGLISH 383     Jewish Literature     22  \n",
       "495      DANCE 262     Congolese Dance I     22  \n",
       "1088    JUDAIC 497         Senior Thesis     21  \n",
       "944    HISTART 399     Independent Study     21  \n",
       "750    ENGLISH 364    Contemporary Novel     21  \n",
       "505    DIGITAL 201   Dig Media Wrtg Mini     21  \n",
       "1903   WRITING 201   Dig Media Wrtg Mini     21  \n",
       "1056       IOE 591        Special Topics     20  \n",
       "1052       IOE 491     Spec Top Ind Engr     20  \n",
       "565      EARTH 532  Sem Climate-Tect-Top     20  \n",
       "542      EARTH 299       Ind Study EARTH     20  \n",
       "1841  THTREMUS 462              Drafting     20  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(user_text, f_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d95d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8aad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf06b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7a9b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = f_21\n",
    "df.drop_duplicates(subset=['course'], inplace=True)\n",
    "df.dropna(subset=['description'], inplace=True)\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f38c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Nbr</th>\n",
       "      <th>course</th>\n",
       "      <th>Term</th>\n",
       "      <th>Session</th>\n",
       "      <th>Acad Group</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>description</th>\n",
       "      <th>Component</th>\n",
       "      <th>Time</th>\n",
       "      <th>...</th>\n",
       "      <th>Seats Remaining</th>\n",
       "      <th>Has WL</th>\n",
       "      <th>Units</th>\n",
       "      <th>sub_title</th>\n",
       "      <th>credits</th>\n",
       "      <th>requirements_distribution</th>\n",
       "      <th>consent</th>\n",
       "      <th>advisory_prerequisites</th>\n",
       "      <th>other_course_info</th>\n",
       "      <th>repeatability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30282</td>\n",
       "      <td>AAS 103</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>Regular Academic Session</td>\n",
       "      <td>Literature, Sci, and the Arts</td>\n",
       "      <td>Afroamerican &amp; African Studies (AAS) Open Sect...</td>\n",
       "      <td>Social Sci Seminar</td>\n",
       "      <td>This course seeks to introduce students to eve...</td>\n",
       "      <td>SEM</td>\n",
       "      <td>1-230PM</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.00</td>\n",
       "      <td>- Reading Africa: Critical Perspectives on Po...</td>\n",
       "      <td>3</td>\n",
       "      <td>SS</td>\n",
       "      <td></td>\n",
       "      <td>Enrollment restricted to first-year students, ...</td>\n",
       "      <td>(Cross-Area Courses). May not be included in a...</td>\n",
       "      <td>May not be repeated for credit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30276</td>\n",
       "      <td>AAS 104</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>Regular Academic Session</td>\n",
       "      <td>Literature, Sci, and the Arts</td>\n",
       "      <td>Afroamerican &amp; African Studies (AAS) Open Sect...</td>\n",
       "      <td>Humanities Seminar</td>\n",
       "      <td>This seminar introduces first-year students to...</td>\n",
       "      <td>SEM</td>\n",
       "      <td>1130-1PM</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.00</td>\n",
       "      <td>- Black Lives and Life Writing: How We Tell S...</td>\n",
       "      <td>3</td>\n",
       "      <td>HU</td>\n",
       "      <td>With permission of instructor.</td>\n",
       "      <td>Enrollment restricted to first-year students, ...</td>\n",
       "      <td>(Cross-Area Courses). May not be included in a...</td>\n",
       "      <td>May not be repeated for credit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19186</td>\n",
       "      <td>AAS 115</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>Regular Academic Session</td>\n",
       "      <td>Literature, Sci, and the Arts</td>\n",
       "      <td>Afroamerican &amp; African Studies (AAS) Open Sect...</td>\n",
       "      <td>Elementary Swahili</td>\n",
       "      <td>This course is an introduction to spoken and w...</td>\n",
       "      <td>REC</td>\n",
       "      <td>1-2PM</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.00</td>\n",
       "      <td>- Swahili Language and Culture</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>May not be repeated for credit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26657</td>\n",
       "      <td>AAS 125</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>Regular Academic Session</td>\n",
       "      <td>Literature, Sci, and the Arts</td>\n",
       "      <td>Afroamerican &amp; African Studies (AAS) Open Sect...</td>\n",
       "      <td>Elem Yoruba I</td>\n",
       "      <td>This course is designed to introduce the Yorub...</td>\n",
       "      <td>REC</td>\n",
       "      <td>9-10AM</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Y</td>\n",
       "      <td>4.00</td>\n",
       "      <td>- Yoruba</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>May not repeat the same language at the same l...</td>\n",
       "      <td>May not be repeated for credit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30898</td>\n",
       "      <td>AAS 202</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>Regular Academic Session</td>\n",
       "      <td>Literature, Sci, and the Arts</td>\n",
       "      <td>Afroamerican &amp; African Studies (AAS) Open Sect...</td>\n",
       "      <td>Intro Afr Diasp Stds</td>\n",
       "      <td>Is the African Diaspora a concept or an actual...</td>\n",
       "      <td>SEM</td>\n",
       "      <td>1-230PM</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.00</td>\n",
       "      <td>- Global Blackness</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>May not be repeated for credit.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class Nbr   course       Term                   Session  \\\n",
       "0       30282  AAS 103  Fall 2021  Regular Academic Session   \n",
       "3       30276  AAS 104  Fall 2021  Regular Academic Session   \n",
       "21      19186  AAS 115  Fall 2021  Regular Academic Session   \n",
       "25      26657  AAS 125  Fall 2021  Regular Academic Session   \n",
       "26      30898  AAS 202  Fall 2021  Regular Academic Session   \n",
       "\n",
       "                       Acad Group  \\\n",
       "0   Literature, Sci, and the Arts   \n",
       "3   Literature, Sci, and the Arts   \n",
       "21  Literature, Sci, and the Arts   \n",
       "25  Literature, Sci, and the Arts   \n",
       "26  Literature, Sci, and the Arts   \n",
       "\n",
       "                                              Subject          Course Title  \\\n",
       "0   Afroamerican & African Studies (AAS) Open Sect...    Social Sci Seminar   \n",
       "3   Afroamerican & African Studies (AAS) Open Sect...    Humanities Seminar   \n",
       "21  Afroamerican & African Studies (AAS) Open Sect...    Elementary Swahili   \n",
       "25  Afroamerican & African Studies (AAS) Open Sect...         Elem Yoruba I   \n",
       "26  Afroamerican & African Studies (AAS) Open Sect...  Intro Afr Diasp Stds   \n",
       "\n",
       "                                          description Component      Time  \\\n",
       "0   This course seeks to introduce students to eve...       SEM   1-230PM   \n",
       "3   This seminar introduces first-year students to...       SEM  1130-1PM   \n",
       "21  This course is an introduction to spoken and w...       REC     1-2PM   \n",
       "25  This course is designed to introduce the Yorub...       REC    9-10AM   \n",
       "26  Is the African Diaspora a concept or an actual...       SEM   1-230PM   \n",
       "\n",
       "    ... Seats Remaining Has WL Units  \\\n",
       "0   ...               1      Y  3.00   \n",
       "3   ...               2      Y  3.00   \n",
       "21  ...              13      Y  4.00   \n",
       "25  ...               5      Y  4.00   \n",
       "26  ...               9      Y  3.00   \n",
       "\n",
       "                                            sub_title credits  \\\n",
       "0    - Reading Africa: Critical Perspectives on Po...       3   \n",
       "3    - Black Lives and Life Writing: How We Tell S...       3   \n",
       "21                     - Swahili Language and Culture       4   \n",
       "25                                           - Yoruba       4   \n",
       "26                                 - Global Blackness       3   \n",
       "\n",
       "   requirements_distribution                         consent  \\\n",
       "0                         SS                                   \n",
       "3                         HU  With permission of instructor.   \n",
       "21                                                             \n",
       "25                                                             \n",
       "26                                                             \n",
       "\n",
       "                               advisory_prerequisites  \\\n",
       "0   Enrollment restricted to first-year students, ...   \n",
       "3   Enrollment restricted to first-year students, ...   \n",
       "21                                                      \n",
       "25                                                      \n",
       "26                                                      \n",
       "\n",
       "                                    other_course_info  \\\n",
       "0   (Cross-Area Courses). May not be included in a...   \n",
       "3   (Cross-Area Courses). May not be included in a...   \n",
       "21                                                      \n",
       "25  May not repeat the same language at the same l...   \n",
       "26                                                      \n",
       "\n",
       "                      repeatability  \n",
       "0   May not be repeated for credit.  \n",
       "3   May not be repeated for credit.  \n",
       "21  May not be repeated for credit.  \n",
       "25  May not be repeated for credit.  \n",
       "26  May not be repeated for credit.  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1463f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[['Class Nbr', 'course', 'Course Title','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395ba893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This course seeks to introduce students to everyday life in urban Africa. The course is designed to equip students with basic and useful knowledge about the how urban residents – rich and poor, newcomers and old-timers, young and old, men and women – negotiate the challenges of living in cities.  This course focuses on networks, associational life, and relationships that are the ties that bind urban residents together.  Social organization, religious belief and practice, ethnicity, economic and political systems, the arts, and popular culture are some of the topics we will explore.  We will be approaching these themes from a variety of disciplinary perspectives, including history, anthropology, literature, political science, sociology, and economics.',\n",
       " 'This seminar introduces first-year students to the intellectual community of humanities scholars working in the field of Afroamerican and African studies.  The topic of the seminar varies from year to year.',\n",
       " 'This course is an introduction to spoken and written Swahili. It is designed to help students learn elementary spoken Swahili and the proper written grammar of the language. The course provides some exposure to the people and cultures of the Swahili-speaking communities.',\n",
       " 'This course is designed to introduce the Yoruba language and culture to non-native speakers.  Emphasis will be put on listening, speaking, reading and writing through communicative activities, enabling the students to perform the various tasks in the target language. Classes will also include role-plays and oral presentations.',\n",
       " 'Is the African Diaspora a concept or an actual geographical location? Is it singular or are there multiple African diasporas? What does diaspora have to do with the multi-lingual, multicultural continent of over fifty countries that make up Africa? What impact has Africa and its diaspora(s) had on the so-called “white” West and its development as a site of tremendous wealth and privilege? AAS 202 engages these questions by exploring the long historical, economic, and political relationships between \"the West\" (e.g., United States, Britain, France, and Germany) and selected countries in Africa and the diaspora (e.g., Jamaica, Haiti, Brazil, Mali, Liberia, Ethiopia, Democratic Republic of Congo, and South Africa). Topics include: pre-colonial African empires; the Middle Passage; child soldiers; public health; conflict minerals; slavery and resistance; migration; empire, colonialism, and post-colonialism; twentieth-century freedom movements; religion; and popular forms of cultural expression.',\n",
       " 'This course concentrates on developing communicative skills to enable the learners engage in meaningful verbal interactions with other Swahili speakers. In order to achieve this goal, most lessons are task-based, both pedagogic and real-life tasks. Activities such as role-play, creating sample materials, discussion, storytelling, describing scenes, and studying authentic cultural objects are used.  Reading and writing passages are carried out as would be in a Swahili speaking community.  Prerequisite: AAS 116 or permission from the instructor after testing.',\n",
       " 'This intermediate level course sequence is designed for students who have successfully completed the Elementary sequence, or with the permission of the instructor. Instruction is offered through a distance-learning, course share program at Indiana University. It broadens speaking, reading and writing skills as students engage in discussions and writing on more complex topics. Videos, audio tapes, newspapers, magazines and the internet provide resources for the content of materials of the students’ interests as well as exposure to more advanced language. They are encouraged to discuss culture as well as news from Africa. Students begin to read short novels, plays, poetry, and essays. Using a variety of materials, students acquire a solid knowledge of morphology and syntax, vocabulary, and more complex practice in speaking and writing. Swahili is currently the only language available at the Intermediate level, but additional languages may be added in the future.',\n",
       " 'The history of early Africa is expansive, encompassing several millennia and several thousands of square miles. It entails the peopling of a continent, the rise of centralized kingdoms and long-distance trade networks, technological innovation, the birth of diverse cultural forms and social identities, violent confrontations, and creative ideas about what it means to live together. It is also a difficult history to access as most African civilizations before the 19th century prioritized unwritten forms of communication. This class presents several important episodes in the long history of early African civilization and introduces students to the ways historians of the distant past think about and use various forms of evidence including language, material culture, oral traditions, and folklore.',\n",
       " 'The course introduces students to the confluence of political and economic forces at the local,\\r\\nnational, regional, and global levels that have helped shape the trajectory of African development.\\r\\nThe course is divided into three parts: the first examines the meaning and evolution of the political\\r\\neconomy of development in the context of Africa’s unfolding history from pre-independence\\r\\nthrough the era of structural adjustment. The second applies an understanding of political economy\\r\\nto topical development issues such as environment and climate change, causes and consequences of\\r\\nconflict, health and development, and international aid. The latter section applies the political\\r\\neconomy framework to particular African country case studies.',\n",
       " 'In this course we will study the emergence and early development of African American literature, from 1773 to the Harlem Renaissance. We will do literary close readings of texts to think through many of the issues relating to the development of an African American literary tradition. What constitutes a tradition? Can there be more than one? Along with these questions, we will examine our own assumptions about this literature and the many ways in which these texts reflect the different experiences of African Americans intent on gaining a national voice in a slaveholding and racially polarized nation.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_1['description'].tolist()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c28e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(np.unique(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc350208",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### Counter Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e689fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f52315ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>121</th>\n",
       "      <th>1945</th>\n",
       "      <th>1980s</th>\n",
       "      <th>1989</th>\n",
       "      <th>19th</th>\n",
       "      <th>2000s</th>\n",
       "      <th>500</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>...</th>\n",
       "      <th>with</th>\n",
       "      <th>within</th>\n",
       "      <th>women</th>\n",
       "      <th>word</th>\n",
       "      <th>write</th>\n",
       "      <th>writing</th>\n",
       "      <th>writings</th>\n",
       "      <th>year</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  12  121  1945  1980s  1989  19th  2000s  500  abandonment  ...  with  \\\n",
       "0   0   2    0     1      1     1     1      1    0            0  ...     6   \n",
       "1   0   0    0     0      0     0     0      0    0            0  ...     0   \n",
       "2   0   0    0     0      0     0     0      0    0            0  ...     3   \n",
       "3   0   0    1     0      0     0     0      0    1            0  ...     0   \n",
       "4   0   0    0     0      0     0     0      0    0            0  ...     0   \n",
       "5   0   0    0     0      0     0     0      0    0            0  ...     4   \n",
       "6   0   0    0     0      0     0     0      0    0            0  ...     1   \n",
       "7   1   1    0     0      0     0     0      0    0            1  ...     3   \n",
       "8   0   0    0     0      0     0     0      0    0            0  ...     0   \n",
       "9   0   0    0     0      0     0     0      0    0            0  ...     1   \n",
       "\n",
       "   within  women  word  write  writing  writings  year  you  your  \n",
       "0       1      0     1      0        0         0     0    0     0  \n",
       "1       0      0     0      0        0         0     0    0     0  \n",
       "2       0      0     0      0        1         0     0    0     0  \n",
       "3       0      0     0      0        1         0     0    0     0  \n",
       "4       0      0     0      0        1         0     0    0     0  \n",
       "5       0      1     0      0        0         0     1    0     0  \n",
       "6       0      0     0      0        0         0     0    0     0  \n",
       "7       0      0     0      1        0         1     0    0     1  \n",
       "8       0      0     0      1        0         0     0    0     0  \n",
       "9       0      0     0      0        1         0     0    1     0  \n",
       "\n",
       "[10 rows x 787 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts = count_vect.fit_transform(corpus[:10])\n",
    "X_train_counts = pd.DataFrame(X_train_counts.toarray())\n",
    "X_train_counts.columns = count_vect.get_feature_names()\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec5c638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nThis course provides an introduction to Polish culture in the larger context of Slavic and Central European cultures through a detailed study and analysis of “music of protest” (jazz, cabaret, rock, punk) during the 1945-1989 period of Soviet dominance and during the period of transition to democracy and after the establishment of full democratic rule in Poland. This course also provides an introduction to rhetoric and contextual reading of poetry (as well as other forms of expression). We will study in detail texts by some of the most important Polish and other Central European pop, jazz, cabaret, rock, and punk authors and bands with the purpose of identifying devices and strategies used to create meanings. We will connect texts with elements of the daily lives of people in Poland (as well as other Central European nations) focusing in particular on: cultural heritage, history, politics, social issues, past and future myths (interpretations of past events and projections of perceived “national goals” into the future). We will study and discuss the “romantic pose” of rock poets (poet as a prophet —  the 19th-century concept created by Polish Romantic poets in the context of constructing art as an instrument of the Polish “battle for freedom and independence”). Specifically, we will investigate the “Polish trend” to build poetic texts on the foundation of ideas (rock poetry as an extension of discourse on ideas). We will compare Polish rock poetry to British and American rock poetry to highlight the dominance of “word and meaning” in Polish rock and punk of the 1980s and 2000s. Other topics will include: \\r\\n\\n totalitarian structures and poetic response to totalitarian structures, \\r\\n the three taboos in Polish culture (communism, Catholicism, anti-communist opposition), \\r\\n the quest for utopia,  \\r\\nthe “apologetic nature” of Polish art,  \\r\\nthe fear of and desire for diversity in Polish culture and history, \\r\\n the construction of a “collective Polish psyche”.\\r\\n\\n This course addresses a wide range of issues related to the study of history, literature, culture, social and cultural transformations, construction and deconstruction of national, social, and gender mythologies and perceptions. It will focus on responses to the imposition of totalitarian structures and survival within them, responses to historical changes, as well as strategies for constructing national consensus and for dealing with a diversity of ideas and attitudes. \\r\\n\\r\\nThis course will focus on Polish culture but will do so in a global context: it will introduce the students to general and foundational concepts and ideas in communication, semiotics, rhetoric, and the reading of poetic texts. The focus on comprehension of complex synthetic art forms in a social, historical, and political context can be applied to other cultures as well (Polish, Slavic, non-Slavic, past, and contemporary). Polish culture will be analyzed in terms of its main building blocks: ethnicity, religion, gender, cultural identity, the importance of ideas and verbal communication, the opposition between the West and the East, the opposition between Enlightenment and Romanticism, opposition between faith and fact, the opposition between real and perceived continuity of history and culture. In the process, Polish culture will be defined in all its distinctiveness but with clear attention to the qualities shared with the cultures of other Slavic and Central European nations and states. Students will analyze how literary and non-literary texts (verbal and non-verbal) are constructed, how meanings are created and transmitted, how real and perceived connections between the author and the audience are developed.\\r\\n\\r\\n  This will allow students to enhance their talents for persuasive and expository presentations and allow them to become “intentional” communicators and receivers of communication. Students will be exposed to the full range of human experiences in the face of war, human rights abuses, totalitarian rule, revolution, and political, social, and cultural transformation.Course Requirements:\\nAttendance; \\r\\nRegular short participation tests; \\r\\nFinal exam. \\r\\n1 argumentative paper (8-12 pages). \\r\\n1 research paper (8-12 pages).\\r\\nIntended Audience:Students seeking to broaden their understanding of foundational concepts in popular culture and its interaction with politics and basic strategies and devices in rhetoric/communication. Students pursuing Slavic majors/minors and seeking to expand their knowledge beyond their primary Slavic field.\\r\\n\\r\\nNo prior knowledge of the Polish language and culture is needed. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a3389af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10          0\n",
       "12          2\n",
       "121         0\n",
       "1945        1\n",
       "1980s       1\n",
       "           ..\n",
       "writing     0\n",
       "writings    0\n",
       "year        0\n",
       "you         0\n",
       "your        0\n",
       "Name: 0, Length: 787, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.loc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c9796",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e707e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>116</th>\n",
       "      <th>1773</th>\n",
       "      <th>19th</th>\n",
       "      <th>202</th>\n",
       "      <th>aas</th>\n",
       "      <th>about</th>\n",
       "      <th>access</th>\n",
       "      <th>achieve</th>\n",
       "      <th>acquire</th>\n",
       "      <th>activities</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>women</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>yoruba</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143477</td>\n",
       "      <td>0.057279</td>\n",
       "      <td>0.096458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.551368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131212</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176424</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080392</td>\n",
       "      <td>0.068341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.125834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125834</td>\n",
       "      <td>0.093586</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181727</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133233</td>\n",
       "      <td>0.089571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228259</td>\n",
       "      <td>0.060750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 446 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        116      1773      19th       202       aas     about    access  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.071739  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.080392  0.068341  0.000000  0.000000   \n",
       "5  0.125834  0.000000  0.000000  0.000000  0.106970  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.089571  0.000000  0.000000  0.133233  0.089571   \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.102304  0.000000  0.000000  0.000000  0.076086  0.000000   \n",
       "\n",
       "    achieve   acquire  activities  ...      will      with     women  \\\n",
       "0  0.000000  0.000000    0.000000  ...  0.143477  0.057279  0.096458   \n",
       "1  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000    0.149977  ...  0.262423  0.000000  0.000000   \n",
       "4  0.000000  0.000000    0.000000  ...  0.000000  0.047738  0.000000   \n",
       "5  0.125834  0.000000    0.106970  ...  0.000000  0.074723  0.000000   \n",
       "6  0.000000  0.081449    0.000000  ...  0.000000  0.048366  0.000000   \n",
       "7  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000    0.000000  ...  0.228259  0.060750  0.000000   \n",
       "\n",
       "    working     would   writing  written      year    yoruba     young  \n",
       "0  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.096458  \n",
       "1  0.183789  0.000000  0.000000  0.00000  0.551368  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.33646  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.131212  0.00000  0.000000  0.176424  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.125834  0.093586  0.00000  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.000000  0.181727  0.00000  0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "8  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "9  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 446 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(corpus[:10])\n",
    "X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray())\n",
    "X_train_tfidf.columns = vectorizer.get_feature_names()\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdbb8d",
   "metadata": {},
   "source": [
    "## Word Vectors  \n",
    "Word vectors - also called word embeddings - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive context.  \n",
    "**There are two possilbe approaches:**  \n",
    "**CBOW (Continuous Bag Of Words):** It predicts the word, given context around the word as input\n",
    "\n",
    "**Skip-gram:** It predicts the context, given the word as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2770e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\program files\\python39\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\program files\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\program files\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python39\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ad9a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\program files\\python39\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (59.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\program files\\python39\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\program files\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\program files\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python39\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.2.0\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b79f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "454df1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp('dog').vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1478b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "      w for w in word.vocab \n",
    "      if w.is_lower == word.is_lower and w.prob >= -30 and np.count_nonzero(w.vector)   #!!!!when I change -30 to -20 no similar words \n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4068a20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lion', 0.47474015),\n",
       " ('he', 0.39728197),\n",
       " ('y.', 0.3512264),\n",
       " ('r.', 0.3512264),\n",
       " ('who', 0.33955762),\n",
       " ('let', 0.33448273),\n",
       " ('when', 0.32163125),\n",
       " ('was', 0.31741124),\n",
       " ('dare', 0.31443095),\n",
       " ('did', 0.31363884)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"king\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e54b2cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.47474015),\n",
       " ('he', 0.31557545),\n",
       " ('i', 0.29816413),\n",
       " ('dare', 0.29063576),\n",
       " ('ca', 0.27542132),\n",
       " ('she', 0.2714466),\n",
       " ('when', 0.26939633),\n",
       " ('u', 0.26731068),\n",
       " ('does', 0.2552375),\n",
       " ('there', 0.2522287)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"lion\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "031769dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.68311125),\n",
       " ('who', 0.56556106),\n",
       " ('when', 0.5414661),\n",
       " ('that', 0.5309426),\n",
       " ('what', 0.51977086),\n",
       " ('she', 0.51776016),\n",
       " ('was', 0.51282316),\n",
       " ('could', 0.5085001),\n",
       " ('there', 0.50046384),\n",
       " ('why', 0.49632692)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"man\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe09abd",
   "metadata": {},
   "source": [
    "Sentence (or document) objects have vectors, derived from the averages of individual token vectors. This makes it possible to compare similarities between whole documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35c26472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I'm interested in Chinese traditional music\")\n",
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b7091cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hEBE2\\AppData\\Local\\Temp\\1/ipykernel_55820/3708747934.py:8: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
      "C:\\Users\\hEBE2\\AppData\\Local\\Temp\\1/ipykernel_55820/3708747934.py:9: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kan', 0.0),\n",
       " ('mar', 0.0),\n",
       " ('sept.', 0.0),\n",
       " ('mont.', 0.0),\n",
       " ('jr.', 0.0),\n",
       " ('k.', 0.0),\n",
       " ('calif.', 0.0),\n",
       " ('ill.', 0.0),\n",
       " (\"o'clock\", 0.0),\n",
       " ('mich.', 0.0),\n",
       " ('might', 0.0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(doc, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d9d2c",
   "metadata": {},
   "source": [
    "## Bert Sentence Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9daaf427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "Collecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python39\\lib\\site-packages (from sentence_transformers) (4.62.3)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.0-cp39-cp39-win_amd64.whl (226.5 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp39-cp39-win_amd64.whl (984 kB)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python39\\lib\\site-packages (from sentence_transformers) (1.21.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\python39\\lib\\site-packages (from sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\program files\\python39\\lib\\site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: nltk in c:\\program files\\python39\\lib\\site-packages (from sentence_transformers) (3.6.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python39\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (4.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.11.10)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in c:\\program files\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python39\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python39\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python39\\lib\\site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
      "Collecting pillow!=8.3.0,>=5.3.0\n",
      "  Downloading Pillow-8.4.0-cp39-cp39-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\program files\\python39\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\program files\\python39\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: six in c:\\program files\\python39\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.16.0)\n",
      "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
      "Installing collected packages: pyyaml, filelock, torch, tokenizers, sacremoses, pillow, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "    Running setup.py install for sentence-transformers: started\n",
      "    Running setup.py install for sentence-transformers: finished with status 'done'\n",
      "Successfully installed filelock-3.4.0 huggingface-hub-0.1.2 pillow-8.4.0 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.0 torchvision-0.11.1 transformers-4.12.5\n"
     ]
    }
   ],
   "source": [
    "#pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "021c14dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 391/391 [00:00<00:00, 392kB/s]\n",
      "Downloading: 100%|██████████| 3.95k/3.95k [00:00<00:00, 661kB/s]\n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 1.00kB/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 313kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 62.4kB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 115kB/s]\n",
      "Downloading: 100%|██████████| 438M/438M [20:47<00:00, 351kB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 17.7kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 37.4kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:01<00:00, 390kB/s]\n",
      "Downloading: 100%|██████████| 399/399 [00:00<00:00, 200kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 367kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 95.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07cbbccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_embeddings = embedder.encode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2193e3e",
   "metadata": {},
   "source": [
    "## Candidate Genration using Faiss vector similarity search library  \n",
    "Faiss is a library developed by Facebook AI Research. It is for effecient similarity search and clustering of dense vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0409e9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.1.post2-cp39-cp39-win_amd64.whl (10.1 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.1.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78496b",
   "metadata": {},
   "source": [
    "Faiss is a library developed by Facebook AI Research. It is for effecient similarity search and clustering of dense vectors.\n",
    "1. Tutorial : https://github.com/facebookresearch/faiss/wiki/Getting-started  \n",
    "2. facebookrearch : https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7440d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1910\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "d= 768\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(np.stack(corpus_embeddings, axis=0))\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37207900",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries =[\"I am interested in computer science\", \"I like pop music\", \n",
    "        \"I like Asian culture, especially Janpanese history\", \"I like to use computer skill to resolve biological problems\",\n",
    "        \"I like playing with data and statistics\"]\n",
    "query_embeddings = embedder.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b5128f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  71  476  479 1468 1796]\n",
      " [ 738  844 1005 1292  339]\n",
      " [ 188  923 1423  457  339]\n",
      " [ 463 1131 1173  671  292]\n",
      " [ 339  457 1423  665  697]]\n"
     ]
    }
   ],
   "source": [
    "k = 5                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(np.stack(query_embeddings, axis=0), k)     # actual search\n",
    "print(I)                   # neighbors of the 5 first queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e894c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Nbr</th>\n",
       "      <th>course</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>21972</td>\n",
       "      <td>ALA 118</td>\n",
       "      <td>Prog, Info &amp; People</td>\n",
       "      <td>Introduction to programming with a focus on ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>32293</td>\n",
       "      <td>COMP 416</td>\n",
       "      <td>Sem Electron Mus</td>\n",
       "      <td>Includes the study of digital synthesis techni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>32295</td>\n",
       "      <td>COMP 526</td>\n",
       "      <td>Adv Stdy Elec Mus</td>\n",
       "      <td>Includes the study of digital synthesis techni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21704</th>\n",
       "      <td>11699</td>\n",
       "      <td>POLSCI 514</td>\n",
       "      <td>Computer Usage</td>\n",
       "      <td>Practical experience in the use of a system of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28181</th>\n",
       "      <td>22083</td>\n",
       "      <td>TCHNCLCM 497</td>\n",
       "      <td>Adv Tch Com for CS</td>\n",
       "      <td>Advanced technical communication for computer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class Nbr        course         Course Title  \\\n",
       "457        21972       ALA 118  Prog, Info & People   \n",
       "5456       32293      COMP 416     Sem Electron Mus   \n",
       "5467       32295      COMP 526    Adv Stdy Elec Mus   \n",
       "21704      11699    POLSCI 514       Computer Usage   \n",
       "28181      22083  TCHNCLCM 497   Adv Tch Com for CS   \n",
       "\n",
       "                                             description  \n",
       "457    Introduction to programming with a focus on ap...  \n",
       "5456   Includes the study of digital synthesis techni...  \n",
       "5467   Includes the study of digital synthesis techni...  \n",
       "21704  Practical experience in the use of a system of...  \n",
       "28181  Advanced technical communication for computer ...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommandation for first query \"I am interested in computer science\"\n",
    "df_1.iloc[I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "81e90d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Nbr</th>\n",
       "      <th>course</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>31586</td>\n",
       "      <td>ENGLISH 319</td>\n",
       "      <td>Lit&amp;Social Change</td>\n",
       "      <td>Sure, pop culture is fun. It’s great to watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13100</th>\n",
       "      <td>31784</td>\n",
       "      <td>FRENCH 272</td>\n",
       "      <td>Fr Film&amp;Culture</td>\n",
       "      <td>In this course, we will explore French-languag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14300</th>\n",
       "      <td>15889</td>\n",
       "      <td>HONORS 135</td>\n",
       "      <td>Ideas in Honors</td>\n",
       "      <td>Music is undoubtedly one of the most powerful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18319</th>\n",
       "      <td>13033</td>\n",
       "      <td>MUSICOL 139</td>\n",
       "      <td>Intro Study Music</td>\n",
       "      <td>A survey of musical concepts and repertories o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>30291</td>\n",
       "      <td>BIOPHYS 445</td>\n",
       "      <td>Intro to Info Theory</td>\n",
       "      <td>This course introduces the basic tools of Info...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class Nbr       course          Course Title  \\\n",
       "11367      31586  ENGLISH 319     Lit&Social Change   \n",
       "13100      31784   FRENCH 272       Fr Film&Culture   \n",
       "14300      15889   HONORS 135       Ideas in Honors   \n",
       "18319      13033  MUSICOL 139     Intro Study Music   \n",
       "3812       30291  BIOPHYS 445  Intro to Info Theory   \n",
       "\n",
       "                                             description  \n",
       "11367  Sure, pop culture is fun. It’s great to watch ...  \n",
       "13100  In this course, we will explore French-languag...  \n",
       "14300  Music is undoubtedly one of the most powerful ...  \n",
       "18319  A survey of musical concepts and repertories o...  \n",
       "3812   This course introduces the basic tools of Info...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recommandation for \"I like pop music\"\n",
    "df_1.iloc[I[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ddabfaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "\n",
      "Query: I am interested in computer science\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Introduction to programming with a focus on applications in informatics.  Covers the fundamental elements of a modern programming language and how to access data on the internet.  Explores how humans and technology complement one another, including techniques used to coordinate groups of people working together on software development. (Distance: 153.2221)\n",
      "Includes the study of digital synthesis techniques. Special attention is given to the relationship between technology, the creative process, and individual statement. (Distance: 160.4367)\n",
      "Includes the study of digital synthesis techniques. Special attention is given to the relationship between technology, the creative process, and individual statement. (Distance: 160.4367)\n",
      "Practical experience in the use of a system of computer programs for social scientists. (Distance: 162.5113)\n",
      "Advanced technical communication for computer science.  Design and writing of user and task analysis, requirements documents, specifications, proposals, reports and documentation, all aimed at diverse organizational audiences. Preparation and delivery of final oral presentations and written project reports.  (Distance: 167.1653)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: I like pop music\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Sure, pop culture is fun. It’s great to watch an Academy Award winner, read a best seller. But stories that sell shape our values and beliefs, change how we act and what we do – often in powerful ways. Given the power of what’s popular, we better understand how that power works. Let’s read some best-selling books, watch a few Academy Award-winning movies, and see what we can figure out. (Distance: 245.6423)\n",
      "In this course, we will explore French-language podcasts, a medium whose popularity has exploded in recent years. While there seems to be a podcast for every subject imaginable, we will give particular attention to personal stories – what are the ethics and implications of telling one’s story via podcast? In what ways does telling one’s story in a podcast to align with or differ from other formats? (Distance: 248.1490)\n",
      "Music is undoubtedly one of the most powerful art forms that exist. It gives us the ability to communicate, to reflect, to express, and to connect. Music joins together nations across the world and people of all backgrounds. But does music have the power to participate in the world of science and physiology and offer real healing? (Distance: 255.0293)\n",
      "A survey of musical concepts and repertories of the Western and non-Western world. (Distance: 277.3761)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 279.7503)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: I like Asian culture, especially Janpanese history\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "This course serves as an introduction to the history and culture of a great city or a group of great cities in Asia (e.g. Tokyo, Seoul, Shanghai, Mumbai). This course focuses on a number of key issues that are critical to making sense of these cities: the relationship between modern and premodern cities, the historical linkages between different Asian cities, and the challenges faced by most Asian cities, including cosmopolitanism, social inequality, suburbanization, pollution, etc. (Distance: 197.2614)\n",
      "In this course, we’ll read a series of works from Greco-Roman antiquity long held to be among the “Great Books” of world literature. We’ll explore the ways in which these texts can still speak to us today because of their remarkably intense focus on central questions of human existence, such as: What makes you who you are? What makes a good life? What is justice? What is your relation to people different from yourself, whether because of their ethnicity, class, or gender? At the same time, however, we’ll ask why these texts, in particular, became “Great Books” because of the answers they provide to these questions, and whether these answers are still our answers. Students will, in addition, have the opportunity to write about modern books and films that are related to the ancient texts but come from diverse perspectives and national traditions. (Distance: 199.7424)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 200.7072)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 200.7072)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 200.7072)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: I like to use computer skill to resolve biological problems\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Artificial agents provide possible models of human cognition, and\n",
      "machine learning represents a substrate upon which models of agents\n",
      "are constructed.  This course is an introduction to machine learning.\n",
      "Although the introduction is general, techniques and applications that\n",
      "arise in natural language processing receive special attention.\n",
      " (Distance: 121.1692)\n",
      "Artificial agents provide possible models of human cognition, and\n",
      "machine learning represents a substrate upon which models of agents\n",
      "are constructed.  This course is an introduction to machine learning.\n",
      "Although the introduction is general, techniques and applications that\n",
      "arise in natural language processing receive special attention.\n",
      " (Distance: 121.1692)\n",
      "From mutants to high-throughput DNA sequencing to targeted genome editing, researchers have developed a powerful set of genetic tools for simple model systems to human populations. This class covers the theory and application of genetic methods from the foundational to the latest technical advances using examples from the primary research literature. (Distance: 142.3947)\n",
      "Basic interdisciplinary concepts needed to implement a microprocessor based control system. Sensors and actuators. Quadrature decoding. Pulse width modulation. DC motors. Force feedback algorithms for human computer interaction. Real time operating systems. Networking. Use of MATLAB to model hybrid dynamical systems. Autocode generation for rapid prototyping. Lecture and laboratory. (Distance: 145.0772)\n",
      "The course introduces fundamental concepts and methods for structural bioinformatics and the advanced applications.  Topics\n",
      "include sequence and structure alignment methods, methods of protein folding and protein structure prediction (homologous modeling, threading an ab intio folding), basics of molecular dynamics and Monte Carlo simulation, principle and application of machine learning, and techniques of protein structure determination (X-ray crystallography NMR and cryo-EM). Emphasis is on the understanding of the concepts plus practical utilization, with the objective to help students use cutting-edge bioinformatics tool/methods to solve problems in their own research. (Distance: 145.3300)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: I like playing with data and statistics\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 183.2810)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 183.2810)\n",
      "This course introduces the basic tools of Information Theory, Entropy, Relative Entropy, and Information, and highlights their utility with applications drawn from various disciplines.  After introducing the basics of probability theory and information theory, we explore topics including coding, data compression, channel capacity, thermodynamics, population dynamics, gene transcription, network science, and more.  Familiarity with a simple computational package is a plus, but not required. (Distance: 183.2810)\n",
      "In the modern world we depend on the efficiency of a myriad of societal networks to transact many activities. This course analyzes them (how they are connected, how they form, and how processes and transactions occur on them) using mathematical tools from graph theory, linear algebra, probability and game theory. (Distance: 195.6104)\n",
      "In the modern world we depend on the efficiency of a myriad of societal networks to transact many activities. This course analyzes them (how they are connected, how they form and how processes and transactions occur on them) using mathematical tools from graph theory, linear algebra, probability and game theory. (Distance: 198.3443)\n"
     ]
    }
   ],
   "source": [
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances, indices = index.search(np.asarray(query_embedding).reshape(1,768),k)\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    for idx in range(0,5):\n",
    "        print(corpus[indices[0,idx]], \"(Distance: %.4f)\" % distances[0,idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba06281",
   "metadata": {},
   "source": [
    "\n",
    "## Reranking using Bidirectional LSTM model\n",
    "**Reference:** https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8518294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "toko_tokenizer = ToktokTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc14b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "        puncts = ['/', ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "         '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "         '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "         '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "         '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "        def clean_text(text):\n",
    "            text = str(text)\n",
    "            text = text.replace('\\n', '')\n",
    "            text = text.replace('\\r', '')\n",
    "            for punct in puncts:\n",
    "                if punct in text:\n",
    "                    text = text.replace(punct, '')\n",
    "            return text.lower()\n",
    "\n",
    "        def clean_numbers(text):\n",
    "            if bool(re.search(r'\\d', text)):\n",
    "                text = re.sub('[0-9]{5,}', '#####', text)\n",
    "                text = re.sub('[0-9]{4}', '####', text)\n",
    "                text = re.sub('[0-9]{3}', '###', text)\n",
    "                text = re.sub('[0-9]{2}', '##', text)\n",
    "            return text\n",
    "\n",
    "        contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "        def _get_contractions(contraction_dict):\n",
    "            contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "            return contraction_dict, contraction_re\n",
    "\n",
    "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "        def replace_contractions(text):\n",
    "            def replace(match):\n",
    "                return contractions[match.group(0)]\n",
    "            return contractions_re.sub(replace, text)\n",
    "\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        def remove_stopwords(text, is_lower_case=True):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            if is_lower_case:\n",
    "                filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "            else:\n",
    "                filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "            filtered_text = ' '.join(filtered_tokens)    \n",
    "            return filtered_text\n",
    "\n",
    "        def lemmatizer(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "        def trim_text(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        def remove_non_english(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [token for token in tokens if d.check(token)]\n",
    "            eng_text = ' '.join(tokens)\n",
    "            return eng_text\n",
    "\n",
    "        text_norm = clean_text(text)\n",
    "        text_norm = clean_numbers(text_norm)\n",
    "        text_norm = replace_contractions(text_norm)\n",
    "#         text_norm = remove_stopwords(text_norm)\n",
    "#         text_norm = remove_non_english(text_norm)\n",
    "        text_norm = lemmatizer(text_norm)\n",
    "        text_norm = trim_text(text_norm)\n",
    "        return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b73e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792306dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
