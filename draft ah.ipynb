{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = pd.read_csv('assets/original/2021-10-19-MichiganOnline-courses.csv')\n",
    "f_21 = pd.read_csv('assets/f_21_merge.csv')\n",
    "w_22 = pd.read_csv('assets/w_22_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Through this course, you will start by address...\n",
       "1      The third and the last course of the Addressin...\n",
       "2      Are you concerned about climate change? Would ...\n",
       "3      This course, Additive Manufacturing, is the th...\n",
       "4      This course builds upon the fundamental concep...\n",
       "                             ...                        \n",
       "516    This module examines the impacts of incarcerat...\n",
       "517    This third course in the “Good with Words: Wri...\n",
       "518    This fourth and final course in the “Good with...\n",
       "519    This second course in the Good with Words: Wri...\n",
       "520    This course will teach you how to use your wri...\n",
       "Name: description, Length: 521, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        This course seeks to introduce students to eve...\n",
       "1        This seminar introduces first-year students to...\n",
       "2        This class explores the fascinating shift in B...\n",
       "3        This seminar introduces first-year students to...\n",
       "4        This seminar introduces first-year students to...\n",
       "                               ...                        \n",
       "66693                                                  NaN\n",
       "66694                                                  NaN\n",
       "66695                                                  NaN\n",
       "66696                                                  NaN\n",
       "66697                                                  NaN\n",
       "Name: description, Length: 66698, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_22['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model_for_each_df(df, num_topics=10):\n",
    "    \n",
    "    data = df['description'].dropna()\n",
    "    \n",
    "    def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        texts_out = []\n",
    "        for text in texts:\n",
    "            doc = nlp(text)\n",
    "            new_text = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    new_text.append(token.lemma_)\n",
    "            final = \" \".join(new_text)\n",
    "            texts_out.append(final)\n",
    "        return (texts_out)\n",
    "\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "\n",
    "    def gen_words(texts):\n",
    "        final = []\n",
    "        for text in texts:\n",
    "            new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "            final.append(new)\n",
    "        return (final)\n",
    "\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "\n",
    "    # Create bigrams and trigrams\n",
    "    bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return([bigram[doc] for doc in texts])\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "    data_bigrams = make_bigrams(data_words)\n",
    "    data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "\n",
    "    # TF-IDF removal\n",
    "    \n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "    low_value = 0.03\n",
    "    words  = []\n",
    "    words_missing_in_tfidf = []\n",
    "    for i in range(0, len(corpus)):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = []\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops = low_value_words+words_missing_in_tfidf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "        corpus[i] = new_bow\n",
    "\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus[:-1],\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=10,\n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=\"auto\")\n",
    "\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_for_each_df(f_21, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
